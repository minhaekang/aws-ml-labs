{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UFO Sightings Implementation and Operations Lab\n",
    "\n",
    "The goal of this notebook is to train and deploy our model into SageMaker online hosting with 1 variant. \n",
    "\n",
    "What we plan on accompishling is the following:\n",
    "1. [Load dataset onto Notebook instance memory from S3](#Step-1:-Load-the-data-from-Amazon-S3)\n",
    "1. [Cleaning, transforming and preparing the dataset](#Step-2:-Cleaning,-transforming-and-preparing-the-dataset)\n",
    "1. [Create and train our model (Linear Learner)](#Step-4:-Creating-and-training-our-model-(Linear-Learner))\n",
    "1. [Deploying the model into SageMaker hosting](#Step-4:-Deploying-the-model-into-SageMaker-hosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's go ahead and import all the needed libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the data from Amazon S3\n",
    "Let's get the UFO sightings data that is stored in S3 and load it into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reportedTimestamp</th>\n",
       "      <th>eventDate</th>\n",
       "      <th>eventTime</th>\n",
       "      <th>shape</th>\n",
       "      <th>duration</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>weather</th>\n",
       "      <th>firstName</th>\n",
       "      <th>lastName</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sighting</th>\n",
       "      <th>physicalEvidence</th>\n",
       "      <th>contact</th>\n",
       "      <th>researchOutcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1977-04-04T04:02:23.340Z</td>\n",
       "      <td>1977-03-31</td>\n",
       "      <td>23:46</td>\n",
       "      <td>circle</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>rain</td>\n",
       "      <td>Ila</td>\n",
       "      <td>Bashirian</td>\n",
       "      <td>47.329444</td>\n",
       "      <td>-122.578889</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1982-11-22T02:06:32.019Z</td>\n",
       "      <td>1982-11-15</td>\n",
       "      <td>22:04</td>\n",
       "      <td>disk</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>Eriberto</td>\n",
       "      <td>Runolfsson</td>\n",
       "      <td>52.664913</td>\n",
       "      <td>-1.034894</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1992-12-07T19:06:52.482Z</td>\n",
       "      <td>1992-12-07</td>\n",
       "      <td>19:01</td>\n",
       "      <td>circle</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>Miller</td>\n",
       "      <td>Watsica</td>\n",
       "      <td>38.951667</td>\n",
       "      <td>-92.333889</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-02-24T21:06:34.898Z</td>\n",
       "      <td>2011-02-21</td>\n",
       "      <td>20:56</td>\n",
       "      <td>disk</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>partly cloudy</td>\n",
       "      <td>Clifton</td>\n",
       "      <td>Bechtelar</td>\n",
       "      <td>41.496944</td>\n",
       "      <td>-71.367778</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1991-03-09T16:18:45.501Z</td>\n",
       "      <td>1991-03-09</td>\n",
       "      <td>11:42</td>\n",
       "      <td>circle</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>mostly cloudy</td>\n",
       "      <td>Jayda</td>\n",
       "      <td>Ebert</td>\n",
       "      <td>47.606389</td>\n",
       "      <td>-122.330833</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>explained</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          reportedTimestamp   eventDate eventTime   shape  duration  \\\n",
       "0  1977-04-04T04:02:23.340Z  1977-03-31     23:46  circle         4   \n",
       "1  1982-11-22T02:06:32.019Z  1982-11-15     22:04    disk         4   \n",
       "2  1992-12-07T19:06:52.482Z  1992-12-07     19:01  circle        49   \n",
       "3  2011-02-24T21:06:34.898Z  2011-02-21     20:56    disk        13   \n",
       "4  1991-03-09T16:18:45.501Z  1991-03-09     11:42  circle        17   \n",
       "\n",
       "   witnesses        weather firstName    lastName   latitude   longitude  \\\n",
       "0          1           rain       Ila   Bashirian  47.329444 -122.578889   \n",
       "1          1  partly cloudy  Eriberto  Runolfsson  52.664913   -1.034894   \n",
       "2          1          clear    Miller     Watsica  38.951667  -92.333889   \n",
       "3          1  partly cloudy   Clifton   Bechtelar  41.496944  -71.367778   \n",
       "4          1  mostly cloudy     Jayda       Ebert  47.606389 -122.330833   \n",
       "\n",
       "  sighting physicalEvidence contact researchOutcome  \n",
       "0        Y                N       N       explained  \n",
       "1        Y                Y       N       explained  \n",
       "2        Y                N       N       explained  \n",
       "3        Y                N       N       explained  \n",
       "4        Y                N       N       explained  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "bucket='mk-ml-labs'\n",
    "sub_folder = 'ufo_dataset'\n",
    "data_key = 'ufo_fullset.csv'\n",
    "data_location = 's3://{}/{}/{}'.format(bucket, sub_folder, data_key)\n",
    "\n",
    "df = pd.read_csv(data_location, low_memory=False)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Cleaning, transforming and preparing the dataset\n",
    "This step is so important. It's crucial that we clean and prepare our data before we do anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Let's go ahead and start preparing our dataset by transforming some of the values into the correct data types. Here is what we are going to take care of.\n",
    "1. Convert the `reportedTimestamp` and `eventDate` to a datetime data types.\n",
    "1. Convert the `shape` and `weather` to a category data type.\n",
    "1. Map the `physicalEvidence` and `contact` from 'Y', 'N' to `0`, `1`.\n",
    "1. Convert the `researchOutcome` to a category data type (target attribute).\n",
    "\n",
    "Let's also drop the columns that are not important. \n",
    "1. We can drop `sighting` becuase it is always 'Y' or Yes. \n",
    "1. Let's drop the `firstName` and `lastName` becuase they are not important in determining the `researchOutcome`.\n",
    "1. Let's drop the `reportedTimestamp` becuase when the sighting was reporting isn't going to help us determine the legitimacy of the sighting.\n",
    "1. We would need to create some sort of buckets for the `eventDate` and `eventTime`, like seasons for example, but since the distribution of dates is pretty even, let's go ahead and drop them.\n",
    "\n",
    "Finally, let's apply one-hot encoding\n",
    "1. We need to one-hot both the `weather` attribute and the `shape` attribute. \n",
    "1. We also need to transform or map the researchOutcome (target) attribute into numeric values. This is what the alogrithm is expecting. We can do this by mapping unexplained, explained, and probable to 0, 1, 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing values with the most common shape\n",
    "df['shape'] = df['shape'].fillna(df['shape'].value_counts().index[0])\n",
    "\n",
    "df['reportedTimestamp'] = pd.to_datetime(df['reportedTimestamp'])\n",
    "df['eventDate'] = pd.to_datetime(df['eventDate'])\n",
    "\n",
    "df['shape'] = df['shape'].astype('category')\n",
    "df['weather'] = df['weather'].astype('category')\n",
    "\n",
    "df['physicalEvidence'] = df['physicalEvidence'].replace({'Y': 1, 'N': 0})\n",
    "df['contact'] = df['contact'].replace({'Y': 1, 'N': 0})\n",
    "\n",
    "df['researchOutcome'] = df['researchOutcome'].astype('category')\n",
    "\n",
    "df.drop(columns=['firstName', 'lastName', 'sighting', 'reportedTimestamp', 'eventDate', 'eventTime'], inplace=True)\n",
    "\n",
    "# Let's one-hot the weather and shape attribute\n",
    "df = pd.get_dummies(df, columns=['weather', 'shape'])\n",
    "\n",
    "# Let's replace the researchOutcome values with 0, 1, 2 for Unexplained, Explained, and Probable\n",
    "df['researchOutcome'] = df['researchOutcome'].replace({'unexplained': 0, 'explained': 1, 'probable': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>duration</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>physicalEvidence</th>\n",
       "      <th>contact</th>\n",
       "      <th>researchOutcome</th>\n",
       "      <th>weather_clear</th>\n",
       "      <th>weather_fog</th>\n",
       "      <th>weather_mostly cloudy</th>\n",
       "      <th>...</th>\n",
       "      <th>weather_stormy</th>\n",
       "      <th>shape_box</th>\n",
       "      <th>shape_circle</th>\n",
       "      <th>shape_disk</th>\n",
       "      <th>shape_light</th>\n",
       "      <th>shape_oval</th>\n",
       "      <th>shape_pyramid</th>\n",
       "      <th>shape_sphere</th>\n",
       "      <th>shape_square</th>\n",
       "      <th>shape_triangle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>47.329444</td>\n",
       "      <td>-122.578889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>52.664913</td>\n",
       "      <td>-1.034894</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>38.951667</td>\n",
       "      <td>-92.333889</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>41.496944</td>\n",
       "      <td>-71.367778</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>47.606389</td>\n",
       "      <td>-122.330833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   duration  witnesses   latitude   longitude  physicalEvidence  contact  \\\n",
       "0         4          1  47.329444 -122.578889                 0        0   \n",
       "1         4          1  52.664913   -1.034894                 1        0   \n",
       "2        49          1  38.951667  -92.333889                 0        0   \n",
       "3        13          1  41.496944  -71.367778                 0        0   \n",
       "4        17          1  47.606389 -122.330833                 0        0   \n",
       "\n",
       "   researchOutcome  weather_clear  weather_fog  weather_mostly cloudy  ...  \\\n",
       "0                1              0            0                      0  ...   \n",
       "1                1              0            0                      0  ...   \n",
       "2                1              1            0                      0  ...   \n",
       "3                1              0            0                      0  ...   \n",
       "4                1              0            0                      1  ...   \n",
       "\n",
       "   weather_stormy  shape_box  shape_circle  shape_disk  shape_light  \\\n",
       "0               0          0             1           0            0   \n",
       "1               0          0             0           1            0   \n",
       "2               0          0             1           0            0   \n",
       "3               0          0             0           1            0   \n",
       "4               0          0             1           0            0   \n",
       "\n",
       "   shape_oval  shape_pyramid  shape_sphere  shape_square  shape_triangle  \n",
       "0           0              0             0             0               0  \n",
       "1           0              0             0             0               0  \n",
       "2           0              0             0             0               0  \n",
       "3           0              0             0             0               0  \n",
       "4           0              0             0             0               0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(18000, 23)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.head())\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Creating and training our model (Linear Learner)\n",
    "\n",
    "Let's evaluate the Linear Learner algorithm as well. Let's go ahead and randomize the data again and get it ready for the Linear Leaner algorithm. We will also rearrange the columns so it is ready for the algorithm (it expects the first column to be the target attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:17: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:20: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "rand_split = np.random.rand(len(df))\n",
    "train_list = rand_split < 0.8\n",
    "val_list = (rand_split >= 0.8) & (rand_split < 0.9)\n",
    "test_list = rand_split >= 0.9\n",
    "\n",
    " # This dataset will be used to train the model.\n",
    "data_train = df[train_list]\n",
    "\n",
    "# This dataset will be used to validate the model.\n",
    "data_val = df[val_list]\n",
    "\n",
    "# This dataset will be used to test the model.\n",
    "data_test = df[test_list]\n",
    "\n",
    "# Breaks the datasets into attribute numpy.ndarray and the same for target attribute.  \n",
    "train_X = data_train.drop(columns='researchOutcome').as_matrix()\n",
    "train_y = data_train['researchOutcome'].as_matrix()\n",
    "\n",
    "val_X = data_val.drop(columns='researchOutcome').as_matrix()\n",
    "val_y = data_val['researchOutcome'].as_matrix()\n",
    "\n",
    "test_X = data_test.drop(columns='researchOutcome').as_matrix()\n",
    "test_y = data_test['researchOutcome'].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, Let's create recordIO file for the training data and upload it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pipe mode recordIO protobuf training data: s3://mk-ml-labs/implementation_operations_lab/linearlearner_train/ufo_sightings_train_recordIO_protobuf.data\n"
     ]
    }
   ],
   "source": [
    "train_file = 'ufo_sightings_train_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_train/{}'.format(train_file)).upload_fileobj(f)\n",
    "training_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_train/{}'.format(bucket, train_file)\n",
    "print('The Pipe mode recordIO protobuf training data: {}'.format(training_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create recordIO file for the validation data and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Pipe mode recordIO protobuf validation data: s3://mk-ml-labs/implementation_operations_lab/linearlearner_validation/ufo_sightings_validatioin_recordIO_protobuf.data\n"
     ]
    }
   ],
   "source": [
    "validation_file = 'ufo_sightings_validatioin_recordIO_protobuf.data'\n",
    "\n",
    "f = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(f, val_X.astype('float32'), val_y.astype('float32'))\n",
    "f.seek(0)\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object('implementation_operations_lab/linearlearner_validation/{}'.format(validation_file)).upload_fileobj(f)\n",
    "validate_recordIO_protobuf_location = 's3://{}/implementation_operations_lab/linearlearner_validation/{}'.format(bucket, validation_file)\n",
    "print('The Pipe mode recordIO protobuf validation data: {}'.format(validate_recordIO_protobuf_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Alright we are good to go for the Linear Learner algorithm. Let's get everything we need from the ECR repository to call the Linear Learner algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "import sagemaker\n",
    "\n",
    "container = get_image_uri(boto3.Session().region_name, 'linear-learner', \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the job name ufo-linear-learner-job-20190923020122\n"
     ]
    }
   ],
   "source": [
    "# Create a training job name\n",
    "job_name = 'ufo-linear-learner-job-{}'.format(datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "print('Here is the job name {}'.format(job_name))\n",
    "\n",
    "# Here is where the model-artifact will be stored\n",
    "output_location = 's3://{}/implementation_operations_lab/linearlearner_output'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we start building out our model by using the SageMaker Python SDK and passing in everything that is required to create a Linear Learner model.\n",
    "\n",
    "First I like to always create a specific job name. Next, we'll need to specify training parameters.\n",
    "\n",
    "Finally, after everything is included and ready, then we can call the `.fit()` function which specifies the S3 location for training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The feature_dim hyperparameter needs to be set to 22.\n"
     ]
    }
   ],
   "source": [
    "print('The feature_dim hyperparameter needs to be set to {}.'.format(data_train.shape[1] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-23 02:01:32 Starting - Starting the training job...\n",
      "2019-09-23 02:01:33 Starting - Launching requested ML instances...\n",
      "2019-09-23 02:02:30 Starting - Preparing the instances for training......\n",
      "2019-09-23 02:03:29 Downloading - Downloading input data......\n",
      "2019-09-23 02:04:30 Training - Training image download completed. Training in progress.\n",
      "2019-09-23 02:04:30 Uploading - Uploading generated training model\n",
      "2019-09-23 02:04:30 Failed - Training job failed\n",
      "\u001b[31mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[31m[09/23/2019 02:04:26 INFO 140129564641088] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'loss_insensitivity': u'0.01', u'epochs': u'15', u'init_bias': u'0.0', u'lr_scheduler_factor': u'auto', u'num_calibration_samples': u'10000000', u'accuracy_top_k': u'3', u'_num_kv_servers': u'auto', u'use_bias': u'true', u'num_point_for_scaler': u'10000', u'_log_level': u'info', u'quantile': u'0.5', u'bias_lr_mult': u'auto', u'lr_scheduler_step': u'auto', u'init_method': u'uniform', u'init_sigma': u'0.01', u'lr_scheduler_minimum_lr': u'auto', u'target_recall': u'0.8', u'num_models': u'auto', u'early_stopping_patience': u'3', u'momentum': u'auto', u'unbias_label': u'auto', u'wd': u'auto', u'optimizer': u'auto', u'_tuning_objective_metric': u'', u'early_stopping_tolerance': u'0.001', u'learning_rate': u'auto', u'_kvstore': u'auto', u'normalize_data': u'true', u'binary_classifier_model_selection_criteria': u'accuracy', u'use_lr_scheduler': u'true', u'target_precision': u'0.8', u'unbias_data': u'auto', u'init_scale': u'0.07', u'bias_wd_mult': u'auto', u'f_beta': u'1.0', u'mini_batch_size': u'1000', u'huber_delta': u'1.0', u'num_classes': u'1', u'beta_1': u'auto', u'loss': u'auto', u'beta_2': u'auto', u'_enable_profiler': u'false', u'normalize_label': u'auto', u'_num_gpus': u'auto', u'balance_multiclass_weights': u'false', u'positive_example_weight_mult': u'1.0', u'l1': u'auto', u'margin': u'1.0'}\u001b[0m\n",
      "\u001b[31m[09/23/2019 02:04:26 INFO 140129564641088] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'loss': u'auto', u'wd': u'0.0002124813912051101', u'optimizer': u'auto', u'predictor_type': u'multiclass_classifier', u'unbias_label': u'auto', u'num_models': u'auto', u'learning_rate': u'-24421901.90670958', u'normalize_data': u'true', u'epochs': u'15', u'normalize_label': u'auto', u'feature_dim': u'22', u'unbias_data': u'auto', u'l1': u'0.06477441539306635', u'mini_batch_size': u'744', u'use_bias': u'ture', u'early_stopping_patience': u'3', u'num_classes': u'3'}\u001b[0m\n",
      "\u001b[31m[09/23/2019 02:04:26 ERROR 140129564641088] Customer Error: The value 'ture' is not valid for the 'use_bias' hyperparameter which accepts one of the following: 'true', 'false', 'TRUE', 'FALSE', 'True', 'False' (caused by ValidationError)\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job ufo-linear-learner-job-20190923020122: Failed. Reason: ClientError: The value 'ture' is not valid for the 'use_bias' hyperparameter which accepts one of the following: 'true', 'false', 'TRUE', 'FALSE', 'True', 'False' (caused by ValidationError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5fedfdf6110c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m'validation'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidate_recordIO_protobuf_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m }\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    883\u001b[0m         \"\"\"\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   1509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   1152\u001b[0m                 ),\n\u001b[1;32m   1153\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m             )\n\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job ufo-linear-learner-job-20190923020122: Failed. Reason: ClientError: The value 'ture' is not valid for the 'use_bias' hyperparameter which accepts one of the following: 'true', 'false', 'TRUE', 'FALSE', 'True', 'False' (caused by ValidationError)"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "# Setup the LinearLeaner algorithm from the ECR container\n",
    "linear = sagemaker.estimator.Estimator(container,\n",
    "                                       role, \n",
    "                                       train_instance_count=1, \n",
    "                                       train_instance_type='ml.c4.xlarge',\n",
    "                                       output_path=output_location,\n",
    "                                       sagemaker_session=sess,\n",
    "                                       input_mode='Pipe')\n",
    "# Setup the hyperparameters\n",
    "linear.set_hyperparameters(feature_dim=22,\n",
    "                           predictor_type='multiclass_classifier',\n",
    "                           num_classes=3\n",
    "                          )\n",
    "\n",
    "# Launch a training job. This method calls the CreateTrainingJob API call\n",
    "data_channels = {\n",
    "    'train': training_recordIO_protobuf_location,\n",
    "    'validation': validate_recordIO_protobuf_location\n",
    "}\n",
    "linear.fit(data_channels, job_name=job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the location of the trained Linear Learner model: s3://mk-ml-labs/implementation_operations_lab/linearlearner_output/ufo-linear-learner-job-20190923020122/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print('Here is the location of the trained Linear Learner model: {}/{}/output/model.tar.gz'.format(output_location, job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we have our trained model we can deploy into production!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Deploying the model into SageMaker hosting\n",
    "\n",
    "Next, let's deploy the model into SageMaker hosting onto a single m4 instance. We can then use this instance to test the model with the test data that we help out at the beginning of the notebook. We can then evaluate things like accuracy, precision, recall, and f1 score. \n",
    "\n",
    "We can use some fancy libraries to build out a confusion matrix/heatmap to see how accurate our model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "multiclass_predictor = linear.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next code is just setup code to allow us to draw out nice and pretty confusion matrix/heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None, \n",
    "                          cmap=None):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "            plt.cm.Greens\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='Actual',\n",
    "           xlabel='Predicted')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import json_deserializer, csv_serializer\n",
    "\n",
    "multiclass_predictor.content_type = 'text/csv'\n",
    "multiclass_predictor.serializer = csv_serializer\n",
    "multiclass_predictor.deserializer = json_deserializer\n",
    "\n",
    "predictions = []\n",
    "results = multiclass_predictor.predict(test_X)\n",
    "predictions += [r['predicted_label'] for r in results['predictions']]\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = predictions\n",
    "\n",
    "class_names = np.array(['Unexplained', 'Explained', 'Probable'])\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix',\n",
    "                      cmap=plt.cm.Blues)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_test = data_test['researchOutcome']\n",
    "y_pred = predictions\n",
    "scores = precision_recall_fscore_support(y_test, y_pred, average='macro', labels=np.unique(y_pred))\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy is: {}'.format(acc))\n",
    "print('Precision is: {}'.format(scores[0]))\n",
    "print('Recall is: {}'.format(scores[1]))\n",
    "print('F1 score is: {}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
